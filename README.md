## Case tÃ©cnico para grupo OboticÃ¡rio

### Objetivo

1. Desenvolvimento de um modelo de previsÃµes sobre os preÃ§os de imÃ³veis a partir de suas caracterÃ­sticas.
2. ImplementaÃ§Ã£o do modelo atavÃ©s de um pipeline de treinamento automatizado e disponibilizaÃ§Ã£o por web service (API)

### Playbook

Este case foi desenvolvido com ferramentas open source e localmente. Portanto Ã© necessÃ¡rio a instalaÃ§Ã£o local dessas
ferramentas para executÃ¡-lo. Segue abaixo os passos necessÃ¡rios para ambiente linux:

1. InstalaÃ§Ã£o Airflow localmente:<br>
    No terminal digite:<br>
        <i>pip install apache-airflow</i><br>
    ApÃ³s a instalaÃ§Ã£o, executar em 2 janelas do terminal:<br>
        <i>airflow webserver --port 8080</i><br>
        <i>airflow scheduler</i><br>
    Obs: essas janelas devem ficar em execuÃ§Ã£o

2. InstalaÃ§Ã£o Docker Desktop<br>
    No terminal digite:<br>
        Para ubuntu: <i>sudo apt-get install ./docker-desktop-amd64.deb</i><br>
        Caso use outro sistema operacional, obtenha mais informaÃ§Ãµes aqui: [aqui](https://www.docker.com/products/docker-desktop/)<br>
    Obs: O Docker desktop deve estar em execuÃ§Ã£o

3. Crie um ambiente virtual e ative-o:<br>
    No terminal digite:<br>

        <i>python -m venv nome_do_ambiente</i><br>

        ou, caso esteja usando python3<br>

        <i>python3 -m venv nome_do_ambiente</i><br>

        Ative-o usando o seguinte comando:<br>

        <i>source nome_do_ambiente/bin/activate</i><br>

4. Instale as dependÃªncias:<br>
    No terminal digite:<br>
        <i>pip install -r requirements.txt</i>

5. Configurando o airflow:<br>
    5.1 criando um novo usuario admin:<br>
        No terminal digite:<br>
            airflow users create \
            --username admin \
            --firstname Peter \
            --lastname Parker \
            --role Admin \
            --email spiderman@superhero.org<br>
        obs: coloque seus dados
    5.2 Acessando airflow:<br>
        No navegador digite:<br>
        <i>http://localhost:8080/</i><br>
        FaÃ§a o login com o usuario criado
    
    5.3 Configurando as dags e plugins<br>
        Adicione as dags do projeto dentro do diretÃ³rio dags onde o airflow foi instalado<br> 
        e o diretÃ³rio plugins na raÃ­z da instalaÃ§Ã£o<br>
    Obs: caso nao exista os diretÃ³rios, crie-os

    5.4 Configurando paths<br>
        Em variÃ¡veis no airflow adicione os seguintes caminhos:<br>
        PROCESSED_DATA_PATH = /caminho_para_o_csv_processado<br>
        ARTEFATOS_PATH = /caminho_para_os_artefatos<br>
        METADADOS_PATH = /caminho_para_os_metadados<br>
        RAW_DATA_PATH = /caminho_para_o_csv_sem_tratamento
    
    6. InstalaÃ§Ã£o mlflow<br>
        No terminal, digite:<br>

        <i>pip install mlflow</i><br>

        Em seguida, deixe em execuÃ§Ã£o o terminal com o seguinte comando:<br>

        <i>mlflow ui --port 5000</i>

    7. Configure o arquivo utils.py<br>
        Dentro do diretÃ³rio plugins, abra o arquivo utils.py e altere a variavel <i>LOCAL_ARTEFATOS</i><br>
        e coloque o caminho local dos artefatos



### Estrutura de diretÃ³rios

```
â””â”€â”€ ğŸ“case-boticario
    â””â”€â”€ ğŸ“app
        â””â”€â”€ main.py
        â””â”€â”€ modelo.py
    â””â”€â”€ ğŸ“artefatos
        â””â”€â”€ ğŸ“metadados
            â””â”€â”€ metadados_v1.json
            â””â”€â”€ metadados_v2.json
            â””â”€â”€ metadados_v3.json
            â””â”€â”€ metadados_v4.json
            â””â”€â”€ metadados_v5.json
            â””â”€â”€ metadados_v6.json
            â””â”€â”€ metadados_v7.json
            â””â”€â”€ metadados_v8.json
            â””â”€â”€ metadados_v9.json
        â””â”€â”€ ğŸ“modelo
            â””â”€â”€ modelo_v1.pkl
            â””â”€â”€ modelo_v2.pkl
            â””â”€â”€ modelo_v3.pkl
            â””â”€â”€ modelo_v4.pkl
            â””â”€â”€ modelo_v5.pkl
            â””â”€â”€ modelo_v6.pkl
            â””â”€â”€ modelo_v7.pkl
            â””â”€â”€ modelo_v8.pkl
            â””â”€â”€ modelo_v9.pkl
        â””â”€â”€ ğŸ“scaler
            â””â”€â”€ scaler_v1.pkl
            â””â”€â”€ scaler_v2.pkl
            â””â”€â”€ scaler_v3.pkl
            â””â”€â”€ scaler_v4.pkl
            â””â”€â”€ scaler_v5.pkl
            â””â”€â”€ scaler_v6.pkl
            â””â”€â”€ scaler_v7.pkl
            â””â”€â”€ scaler_v8.pkl
            â””â”€â”€ scaler_v9.pkl
    â””â”€â”€ ğŸ“dados
        â””â”€â”€ ğŸ“processed
            â””â”€â”€ HousePrices_HalfMil_processed.csv
        â””â”€â”€ ğŸ“raw
            â””â”€â”€ HousePrices_HalfMil.csv
    â””â”€â”€ ğŸ“dags
        â””â”€â”€ build_deploy_infra.py
        â””â”€â”€ extrai_trata_dados.py
        â””â”€â”€ treina_salva_modelo.py
    â””â”€â”€ Docs
        â””â”€â”€ arquitetura_case_boticario.png
    â””â”€â”€ ğŸ“notebooks
        â””â”€â”€ model.ipynb
    â””â”€â”€ ğŸ“plugins
        â””â”€â”€ __init__.py
        â””â”€â”€ ğŸ“__pycache__
            â””â”€â”€ __init__.cpython-311.pyc
            â””â”€â”€ utils.cpython-311.pyc
        â””â”€â”€ utils.py
    â””â”€â”€ .gitignore
    â””â”€â”€ cliente.py
    â””â”€â”€ docker-compose.yml
    â””â”€â”€ Dockerfile
    â””â”€â”€ README.md
    â””â”€â”€ requirements.txt
```
### Arquivos e diretÃ³rios

1. app: diretÃ³rio onde estÃ£o os arquivos de configuraÃ§Ã£o da API<br>
2. artefatos: diretÃ³rio onde estÃ£o os artefatos gerados no treinamento do modelo<br>
3. dados: diretÃ³rio onde estÃ£o os arquivos CSV raw e processado<br>
4. dags: diretÃ³rio onde se encontram as dags de treinamento e deploy do modelo<br>
    4.1 build_deploy_infra.py: Cria imagem e contÃ¢iner docker para a API<br>
    4.2 extrai_trata_dados.py: Extrai os dados brutos do csv em /dados/raw, <br>
    trata os dados e salva em /dados/processed<br>
    4.3 treina_salva_modelo.py Treina modelo, checa mÃ©tricas e salva em disco em /artefatos<br>
5. docs: arquivos de documentaÃ§Ã£o <br>
6. notebooks: notebook com anÃ¡lise, exploraÃ§Ã£o de dados e experimentaÃ§Ã£o do modelo<br>
7. plugins: arquivos de configuraÃ§Ã£o e suporte as dags<br>
    7.1: utils.py: arquivo contem funÃ§Ãµes de suporte para dag de treinamento e de carragamento do modelo no cont&ainer
8. cliente.py: arquivo que realiza requests a API<br>
9. docker-compose.yml: configuraÃ§Ã£o de imagem e contÃ¢iners<br>
10. Dockerfile: configuraÃ§Ã£o de arquivos e comportamento do contÃ¢iner<br>
11. requirements.txt: bibliotecas de dependÃªncias 

### Arquitetura

![Logo do Projeto](docs/arquitetura_case_boticario.png)

### Tecnologias usadas

1. python<br>
2. bash Script<br>
3. airflow<br>
4. docker<br>
5. mlflow<br>
6. linux<br>


### ConclusÃ£o e prÃ³ximos passos

O projeto atende a necessidade de disponibilizar um modelo de machine learning atravÃ©s de um pipeline de dados.<br>
Utiliza de tecnologias e processos simplificados, de fÃ¡cil compreensÃ£o e eficaz para o volume de dados.<br>
Em caso de maior volumetria, alguns processos poderiam ser melhorados como:<br>
- Utilizar serviÃ§os em nuvem como Composer para Airflow, storage para armazenamento dos arquivos, um<br>
cluster Dataproc com pyspark para ingestÃ£o e processamento de dados ou atÃ© mesmo usar Cloud functions com
triggers de mensageria do Pub/sub<br>
- Os dados poderiam ser armazenados em banco de dados e poderÃ­amos usar scripts em SQL para ingestÃ£o,<br>
utilizando ferramentas como pyspark.sql<br>
- A infraestrutura poderia estar em IaC(terraform) para um melhor controle e automaÃ§Ã£o dos recursos




